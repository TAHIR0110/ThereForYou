{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model,load_model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras import metrics, backend as K\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For use with truncated metrics,\n",
    "# take maxlen from the validation set.\n",
    "# Hacky and hard-coded for now.\n",
    "VAL_MAXLEN = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_acc(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    acc = metrics.categorical_accuracy(y_true, y_pred)\n",
    "    return K.mean(acc, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_loss(y_true, y_pred):\n",
    "    y_true = y_true[:, :VAL_MAXLEN, :]\n",
    "    y_pred = y_pred[:, :VAL_MAXLEN, :]\n",
    "    \n",
    "    loss = K.categorical_crossentropy(\n",
    "        target=y_true, output=y_pred, from_logits=False)\n",
    "    return K.mean(loss, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq(hidden_size, nb_input_chars, nb_target_chars):\n",
    "\n",
    "    \n",
    "    # Define the main model consisting of encoder and decoder.\n",
    "    encoder_inputs = Input(shape=(None, nb_input_chars),\n",
    "                           name='encoder_data')\n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=True, return_state=False,\n",
    "                        name='encoder_lstm_1')\n",
    "    encoder_outputs = encoder_lstm(encoder_inputs)\n",
    "    \n",
    "    encoder_lstm = LSTM(hidden_size, recurrent_dropout=0.2,\n",
    "                        return_sequences=False, return_state=True,\n",
    "                        name='encoder_lstm_2')\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_outputs)\n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "\n",
    "    # Set up the decoder, using `encoder_states` as initial state.\n",
    "    decoder_inputs = Input(shape=(None, nb_target_chars),\n",
    "                           name='decoder_data')\n",
    "    # We set up our decoder to return full output sequences,\n",
    "    # and to return internal states as well. We don't use the return\n",
    "    # states in the training model, but we will use them in inference.\n",
    "    decoder_lstm = LSTM(hidden_size, dropout=0.2, return_sequences=True,\n",
    "                        return_state=True, name='decoder_lstm')\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                         initial_state=encoder_states)\n",
    "    decoder_softmax = Dense(nb_target_chars, activation='softmax',\n",
    "                            name='decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "\n",
    "    # The main model will turn `encoder_input_data` & `decoder_input_data`\n",
    "    # into `decoder_target_data`\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                  outputs=decoder_outputs)\n",
    "    \n",
    "    adam = Adam(lr=0.001, decay=0.0)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy', truncated_acc, truncated_loss])\n",
    "    \n",
    "    # Define the encoder model separately.\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    # Define the decoder model separately.\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "\n",
    "    return model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unidecode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = '\\t' # start of sequence.\n",
    "EOS = '*' # end of sequence.\n",
    "CHARS = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ ')\n",
    "REMOVE_CHARS = '[#$%\"\\+@<=>!&,-.?:;()*\\[\\]^_`{|}~/\\d\\t\\n\\r\\x0b\\x0c]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one-hot integer representation\n",
    "    + Decode the one-hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "          chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char2index = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.index2char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.size = len(self.chars)\n",
    "    \n",
    "    def encode(self, C, nb_rows):\n",
    "        \"\"\"One-hot encode given string C.\n",
    "        # Arguments\n",
    "          C: string, to be encoded.\n",
    "          nb_rows: Number of rows in the returned one-hot encoding. This is\n",
    "          used to keep the # of rows for each data the same via padding.\n",
    "        \"\"\"\n",
    "        x = np.zeros((nb_rows, len(self.chars)), dtype=np.float32)\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char2index[c]] = 1.0\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"Decode the given vector or 2D array to their character output.\n",
    "        # Arguments\n",
    "          x: A vector or 2D array of probabilities or one-hot encodings,\n",
    "          or a vector of character indices (used with `calc_argmax=False`).\n",
    "          calc_argmax: Whether to find the character index with maximum\n",
    "          probability, defaults to `True`.\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            indices = x.argmax(axis=-1)\n",
    "        else:\n",
    "            indices = x\n",
    "        chars = ''.join(self.index2char[ind] for ind in indices)\n",
    "        return indices, chars\n",
    "\n",
    "    def sample_multinomial(self, preds, temperature=1.0):\n",
    "        \"\"\"Sample index and character output from `preds`,\n",
    "        an array of softmax probabilities with shape (1, 1, nb_chars).\n",
    "        \"\"\"\n",
    "        # Reshaped to 1D array of shape (nb_chars,).\n",
    "        preds = np.reshape(preds, len(self.chars)).astype(np.float64)\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probs = np.random.multinomial(1, preds, 1)\n",
    "        index = np.argmax(probs)\n",
    "        char  = self.index2char[index]\n",
    "        return index, char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(data_path, list_of_books):\n",
    "    text = ''\n",
    "    for book in list_of_books:\n",
    "        file_path = os.path.join(data_path, book)\n",
    "        strings = unidecode.unidecode(open(file_path).read())\n",
    "        text += strings + ' '\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [re.sub(REMOVE_CHARS, '', token)\n",
    "              for token in re.split(\"[-\\n ]\", text)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_speling_erors(token, error_rate):\n",
    "    \"\"\"Simulate some artificial spelling mistakes.\"\"\"\n",
    "    assert(0.0 <= error_rate < 1.0)\n",
    "    if len(token) < 3:\n",
    "        return token\n",
    "    rand = np.random.rand()\n",
    "    # Here are 4 different ways spelling mistakes can occur,\n",
    "    # each of which has equal chance.\n",
    "    prob = error_rate / 4.0\n",
    "    if rand < prob:\n",
    "        # Replace a character with a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index + 1:]\n",
    "    elif prob < rand < prob * 2:\n",
    "        # Delete a character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + token[random_char_index + 1:]\n",
    "    elif prob * 2 < rand < prob * 3:\n",
    "        # Add a random character.\n",
    "        random_char_index = np.random.randint(len(token))\n",
    "        token = token[:random_char_index] + np.random.choice(CHARS) \\\n",
    "                + token[random_char_index:]\n",
    "    elif prob * 3 < rand < prob * 4:\n",
    "        # Transpose 2 characters.\n",
    "        random_char_index = np.random.randint(len(token) - 1)\n",
    "        token = token[:random_char_index]  + token[random_char_index + 1] \\\n",
    "                + token[random_char_index] + token[random_char_index + 2:]\n",
    "    else:\n",
    "        # No spelling errors.\n",
    "        pass\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(tokens, maxlen, error_rate=0.3, shuffle=True):\n",
    "    \"\"\"Transform tokens into model inputs and targets.\n",
    "    All inputs and targets are padded to maxlen with EOS character.\n",
    "    \"\"\"\n",
    "    if shuffle:\n",
    "        print('Shuffling data.')\n",
    "        np.random.shuffle(tokens)\n",
    "    encoder_tokens = []\n",
    "    decoder_tokens = []\n",
    "    target_tokens = []\n",
    "    for token in tokens:\n",
    "        encoder = add_speling_erors(token, error_rate=error_rate)\n",
    "        encoder += EOS * (maxlen - len(encoder)) # Padded to maxlen.\n",
    "        encoder_tokens.append(encoder)\n",
    "    \n",
    "        decoder = SOS + token\n",
    "        decoder += EOS * (maxlen - len(decoder))\n",
    "        decoder_tokens.append(decoder)\n",
    "    \n",
    "        target = decoder[1:]\n",
    "        target += EOS * (maxlen - len(target))\n",
    "        target_tokens.append(target)\n",
    "        \n",
    "        assert(len(encoder) == len(decoder) == len(target))\n",
    "    return encoder_tokens, decoder_tokens, target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(tokens, maxlen, ctable, batch_size=128, reverse=False):\n",
    "    \"\"\"Split data into chunks of `batch_size` examples.\"\"\"\n",
    "    def generate(tokens, reverse):\n",
    "        while(True): # This flag yields an infinite generator.\n",
    "            for token in tokens:\n",
    "                if reverse:\n",
    "                    token = token[::-1]\n",
    "                yield token\n",
    "    \n",
    "    token_iterator = generate(tokens, reverse)\n",
    "    data_batch = np.zeros((batch_size, maxlen, ctable.size),\n",
    "                          dtype=np.float32)\n",
    "    while(True):\n",
    "        for i in range(batch_size):\n",
    "            token = next(token_iterator)\n",
    "            data_batch[i] = ctable.encode(token, maxlen)\n",
    "        yield data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datagen(encoder_iter, decoder_iter, target_iter):\n",
    "    \"\"\"Utility function to load data into required model format.\"\"\"\n",
    "    inputs = zip(encoder_iter, decoder_iter)\n",
    "    while(True):\n",
    "        encoder_input, decoder_input = next(inputs)\n",
    "        target = next(target_iter)\n",
    "        yield ([encoder_input, decoder_input], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(inputs, targets, input_ctable, target_ctable,\n",
    "                     maxlen, reverse, encoder_model, decoder_model,\n",
    "                     nb_examples, sample_mode='argmax', random=True):\n",
    "    input_tokens = []\n",
    "    target_tokens = []\n",
    "    \n",
    "    if random:\n",
    "        indices = np.random.randint(0, len(inputs), nb_examples)\n",
    "    else:\n",
    "        indices = range(nb_examples)\n",
    "        \n",
    "    for index in indices:\n",
    "        input_tokens.append(inputs[index])\n",
    "        target_tokens.append(targets[index])\n",
    "    input_sequences = batch(input_tokens, maxlen, input_ctable,\n",
    "                            nb_examples, reverse)\n",
    "    input_sequences = next(input_sequences)\n",
    "    \n",
    "    # Procedure for inference mode (sampling):\n",
    "    # 1) Encode input and retrieve initial decoder state.\n",
    "    # 2) Run one step of decoder with this initial state\n",
    "    #    and a start-of-sequence character as target.\n",
    "    #    Output will be the next target character.\n",
    "    # 3) Repeat with the current target character and current states.\n",
    "\n",
    "    # Encode the input as state vectors.    \n",
    "    states_value = encoder_model.predict(input_sequences)\n",
    "    \n",
    "    # Create batch of empty target sequences of length 1 character.\n",
    "    target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "    # Populate the first element of target sequence\n",
    "    # with the start-of-sequence character.\n",
    "    target_sequences[:, 0, target_ctable.char2index[SOS]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences.\n",
    "    # Exit condition: either hit max character limit\n",
    "    # or encounter end-of-sequence character.\n",
    "    decoded_tokens = [''] * nb_examples\n",
    "    for _ in range(maxlen):\n",
    "        # `char_probs` has shape\n",
    "        # (nb_examples, 1, nb_target_chars)\n",
    "        char_probs, h, c = decoder_model.predict(\n",
    "            [target_sequences] + states_value)\n",
    "\n",
    "        # Reset the target sequences.\n",
    "        target_sequences = np.zeros((nb_examples, 1, target_ctable.size))\n",
    "\n",
    "        # Sample next character using argmax or multinomial mode.\n",
    "        sampled_chars = []\n",
    "        for i in range(nb_examples):\n",
    "            if sample_mode == 'argmax':\n",
    "                next_index, next_char = target_ctable.decode(\n",
    "                    char_probs[i], calc_argmax=True)\n",
    "            elif sample_mode == 'multinomial':\n",
    "                next_index, next_char = target_ctable.sample_multinomial(\n",
    "                    char_probs[i], temperature=0.5)\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    \"`sample_mode` accepts `argmax` or `multinomial`.\")\n",
    "            decoded_tokens[i] += next_char\n",
    "            sampled_chars.append(next_char) \n",
    "            # Update target sequence with index of next character.\n",
    "            target_sequences[i, 0, next_index] = 1.0\n",
    "\n",
    "        stop_char = set(sampled_chars)\n",
    "        if len(stop_char) == 1 and stop_char.pop() == EOS:\n",
    "            break\n",
    "            \n",
    "        # Update states.\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    # Sampling finished.\n",
    "    input_tokens   = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in input_tokens]\n",
    "    target_tokens  = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in target_tokens]\n",
    "    decoded_tokens = [re.sub('[%s]' % EOS, '', token)\n",
    "                      for token in decoded_tokens]\n",
    "    return input_tokens, target_tokens, decoded_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restore_model(path_to_full_model, hidden_size):\n",
    "    \"\"\"Restore model to construct the encoder and decoder.\"\"\"\n",
    "    model = load_model(path_to_full_model, custom_objects={\n",
    "        'truncated_acc': truncated_acc, 'truncated_loss': truncated_loss})\n",
    "    \n",
    "    encoder_inputs = model.input[0] # encoder_data\n",
    "    encoder_lstm1 = model.get_layer('encoder_lstm_1')\n",
    "    encoder_lstm2 = model.get_layer('encoder_lstm_2')\n",
    "    \n",
    "    encoder_outputs = encoder_lstm1(encoder_inputs)\n",
    "    _, state_h, state_c = encoder_lstm2(encoder_outputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_states)\n",
    "\n",
    "    decoder_inputs = model.input[1] # decoder_data\n",
    "    decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "    decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_lstm = model.get_layer('decoder_lstm')\n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_inputs, initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_softmax = model.get_layer('decoder_softmax')\n",
    "    decoder_outputs = decoder_softmax(decoder_outputs)\n",
    "    decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                          outputs=[decoder_outputs] + decoder_states)\n",
    "    return encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rate = 0.8\n",
    "hidden_size = 512\n",
    "nb_epochs = 100\n",
    "train_batch_size = 128\n",
    "val_batch_size = 256\n",
    "sample_mode = 'argmax'\n",
    "# Input sequences may optionally be reversed,\n",
    "# shown to increase performance by introducing\n",
    "# shorter term dependencies between source and target:\n",
    "# \"Learning to Execute\"\n",
    "# http://arxiv.org/abs/1410.4615\n",
    "# \"Sequence to Sequence Learning with Neural Networks\"\n",
    "# https://arxiv.org/abs/1409.3215\n",
    "reverse = True\n",
    "\n",
    "data_path = '../input/spelling'\n",
    "train_books = ['aspell.txt', 'big.txt',\n",
    "               'birkbeck.txt', 'wikipedia.txt']\n",
    "val_books = ['spell-testset1.txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data.\n",
    "text  = read_text(data_path, train_books)\n",
    "vocab = tokenize(text)\n",
    "vocab = list(filter(None, set(vocab)))\n",
    "    \n",
    "# `maxlen` is the length of the longest word in the vocabulary\n",
    "# plus two SOS and EOS characters.\n",
    "maxlen = max([len(token) for token in vocab]) + 2\n",
    "train_encoder, train_decoder, train_target = transform(\n",
    "    vocab, maxlen, error_rate=error_rate, shuffle=False)\n",
    "print(train_encoder[:10])\n",
    "print(train_decoder[:10])\n",
    "print(train_target[:10])\n",
    "\n",
    "input_chars = set(' '.join(train_encoder))\n",
    "target_chars = set(' '.join(train_decoder))\n",
    "nb_input_chars = len(input_chars)\n",
    "nb_target_chars = len(target_chars)\n",
    "\n",
    "print('Size of training vocabulary =', len(vocab))\n",
    "print('Number of unique input characters:', nb_input_chars)\n",
    "print('Number of unique target characters:', nb_target_chars)\n",
    "print('Max sequence length in the training set:', maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "['satifamtoin*********************************', 'svaagly*************************************', 'therouglhy**********************************', 'progroR*************************************', 'collects************************************', 'tenemeWt************************************', 'jranker*************************************', 'blotting************************************', 'voWye***************************************', 'ansiaIzaty**********************************']\n",
    "['\\tsatifactoin********************************', '\\tsavagly************************************', '\\ttheroughly*********************************', '\\tprogrom************************************', '\\tcollects***********************************', '\\ttenement***********************************', '\\tranker*************************************', '\\tblotting***********************************', '\\tvolye**************************************', '\\tansiazaty**********************************']\n",
    "['satifactoin*********************************', 'savagly*************************************', 'theroughly**********************************', 'progrom*************************************', 'collects************************************', 'tenement************************************', 'ranker**************************************', 'blotting************************************', 'volye***************************************', 'ansiazaty***********************************']\n",
    "Size of training vocabulary = 71874\n",
    "Number of unique input characters: 55\n",
    "Number of unique target characters: 56\n",
    "Max sequence length in the training set: 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data.\n",
    "text = read_text(data_path, val_books)\n",
    "val_tokens = tokenize(text)\n",
    "val_tokens = list(filter(None, val_tokens))\n",
    "\n",
    "val_maxlen = max([len(token) for token in val_tokens]) + 2\n",
    "val_encoder, val_decoder, val_target = transform(\n",
    "    val_tokens, maxlen, error_rate=error_rate, shuffle=False)\n",
    "print(val_encoder[:10])\n",
    "print(val_decoder[:10])\n",
    "print(val_target[:10])\n",
    "print('Number of non-unique validation tokens =', len(val_tokens))\n",
    "print('Max sequence length in the validation set:', val_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "['cotnented***********************************', 'contnepted**********************************', 'contene*************************************', 'conended************************************', 'contentid***********************************', 'beginnin************************************', 'egining*************************************', 'preblem*************************************', 'prolam**************************************', 'porble**************************************']\n",
    "['\\tcontented**********************************', '\\tcontenpted*********************************', '\\tcontende***********************************', '\\tcontended**********************************', '\\tcontentid**********************************', '\\tbeginning**********************************', '\\tbegining***********************************', '\\tproblem************************************', '\\tproblam************************************', '\\tproble*************************************']\n",
    "['contented***********************************', 'contenpted**********************************', 'contende************************************', 'contended***********************************', 'contentid***********************************', 'beginning***********************************', 'begining************************************', 'problem*************************************', 'problam*************************************', 'proble**************************************']\n",
    "Number of non-unique validation tokens = 411\n",
    "Max sequence length in the validation set: 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Define training and evaluation configuration.\n",
    "input_ctable  = CharacterTable(input_chars)\n",
    "target_ctable = CharacterTable(target_chars)\n",
    "\n",
    "train_steps = len(vocab) // train_batch_size\n",
    "val_steps = len(val_tokens) // val_batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model.\n",
    "model, encoder_model, decoder_model = seq2seq(\n",
    "    hidden_size, nb_input_chars, nb_target_chars)\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
    "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
    "Model: \"model_1\"\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "encoder_data (InputLayer)       [(None, None, 55)]   0                                            \n",
    "__________________________________________________________________________________________________\n",
    "encoder_lstm_1 (LSTM)           (None, None, 512)    1163264     encoder_data[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "decoder_data (InputLayer)       [(None, None, 56)]   0                                            \n",
    "__________________________________________________________________________________________________\n",
    "encoder_lstm_2 (LSTM)           [(None, 512), (None, 2099200     encoder_lstm_1[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "decoder_lstm (LSTM)             [(None, None, 512),  1165312     decoder_data[0][0]               \n",
    "                                                                 encoder_lstm_2[0][1]             \n",
    "                                                                 encoder_lstm_2[0][2]             \n",
    "__________________________________________________________________________________________________\n",
    "decoder_softmax (Dense)         (None, None, 56)     28728       decoder_lstm[0][0]               \n",
    "==================================================================================================\n",
    "Total params: 4,456,504\n",
    "Trainable params: 4,456,504\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate.\n",
    "for epoch in range(nb_epochs):\n",
    "    print('Main Epoch {:d}/{:d}'.format(epoch + 1, nb_epochs))\n",
    "    \n",
    "    train_encoder, train_decoder, train_target = transform(\n",
    "            vocab, maxlen, error_rate=error_rate, shuffle=True)\n",
    "        \n",
    "    train_encoder_batch = batch(train_encoder, maxlen, input_ctable,\n",
    "                                    train_batch_size, reverse)\n",
    "    train_decoder_batch = batch(train_decoder, maxlen, target_ctable,\n",
    "                                    train_batch_size)\n",
    "    train_target_batch  = batch(train_target, maxlen, target_ctable,\n",
    "                                    train_batch_size)    \n",
    "\n",
    "    val_encoder_batch = batch(val_encoder, maxlen, input_ctable,\n",
    "                                  val_batch_size, reverse)\n",
    "    val_decoder_batch = batch(val_decoder, maxlen, target_ctable,\n",
    "                                  val_batch_size)\n",
    "    val_target_batch  = batch(val_target, maxlen, target_ctable,\n",
    "                                  val_batch_size)\n",
    "    \n",
    "    train_loader = datagen(train_encoder_batch,\n",
    "                               train_decoder_batch, train_target_batch)\n",
    "    val_loader = datagen(val_encoder_batch,\n",
    "                             val_decoder_batch, val_target_batch)\n",
    "    \n",
    "    model.fit_generator(train_loader,\n",
    "                        steps_per_epoch=train_steps,\n",
    "                        epochs=1, verbose=1,\n",
    "                        validation_data=val_loader,\n",
    "                        validation_steps=val_steps)\n",
    "\n",
    "    # On epoch end - decode a batch of misspelled tokens from the\n",
    "    # validation set to visualize speller performance.\n",
    "    nb_tokens = 5\n",
    "    input_tokens, target_tokens, decoded_tokens = decode_sequences(\n",
    "        val_encoder, val_target, input_ctable, target_ctable,\n",
    "        maxlen, reverse, encoder_model, decoder_model, nb_tokens,\n",
    "        sample_mode=sample_mode, random=True)\n",
    "        \n",
    "    print('-')\n",
    "    print('Input tokens:  ', input_tokens)\n",
    "    print('Decoded tokens:', decoded_tokens)\n",
    "    print('Target tokens: ', target_tokens)\n",
    "    print('-')\n",
    "        \n",
    "    # Save the model at end of each epoch.\n",
    "    model_file = '_'.join(['seq2seq', 'epoch', str(epoch + 1)]) + '.h5'\n",
    "    save_dir = 'checkpoints'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_path = os.path.join(save_dir, model_file)\n",
    "    print('Saving full model to {:s}'.format(save_path))\n",
    "    model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Main Epoch 1/100\n",
    "Shuffling data.\n",
    "/opt/conda/lib/python3.7/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
    "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
    "2022-04-10 01:54:57.284613: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
    "2022-04-10 01:55:03.868153: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n",
    "561/561 [==============================] - 192s 330ms/step - loss: 0.4673 - accuracy: 0.8697 - truncated_acc: 0.6450 - truncated_loss: 1.2510 - val_loss: 0.1629 - val_accuracy: 0.9524 - val_truncated_acc: 0.8694 - val_truncated_loss: 0.4478\n",
    "-\n",
    "Input tokens:   ['soVmone', 'evxtended', 'minutes', 'chzlenges', 'stumache']\n",
    "Decoded tokens: ['soomen', 'extented', 'minters', 'chlenges', 'stumache']\n",
    "Target tokens:  ['somone', 'extended', 'minutes', 'chalenges', 'stumache']\n",
    "-\n",
    "Saving full model to checkpoints/seq2seq_epoch_1.h5\n",
    "Main Epoch 2/100\n",
    "Shuffling data.\n",
    "561/561 [==============================] - 185s 330ms/step - loss: 0.1544 - accuracy: 0.9541 - truncated_acc: 0.8738 - truncated_loss: 0.4233 - val_loss: 0.0903 - val_accuracy: 0.9764 - val_truncated_acc: 0.9351 - val_truncated_loss: 0.2482\n",
    "-\n",
    "Input tokens:   ['centraly', 'opnisite', 'aDess', 'wotve', 'hierchky']\n",
    "Decoded tokens: ['centrally', 'opisitie', 'aless', 'wotev', 'hirechy']\n",
    "Target tokens:  ['centraly', 'oppisite', 'acess', 'wote', 'hierchy']\n",
    "-\n",
    "Saving full model to checkpoints/seq2seq_epoch_2.h5\n",
    "Main Epoch 3/100\n",
    "Shuffling data.\n",
    "561/561 [==============================] - 183s 326ms/step - loss: 0.1185 - accuracy: 0.9653 - truncated_acc: 0.9046 - truncated_loss: 0.3256 - val_loss: 0.0814 - val_accuracy: 0.9783 - val_truncated_acc: 0.9404 - val_truncated_loss: 0.2238\n",
    "-\n",
    "Input tokens:   ['pZretend', 'agh', 'possibl', 'supersed', 'neccesayr']\n",
    "Decoded tokens: ['pretend', 'agh', 'possibl', 'supersed', 'neccesary']\n",
    "Target tokens:  ['pretend', 'lagh', 'possible', 'supersede', 'neccesary']\n",
    "-\n",
    "Saving full model to checkpoints/seq2seq_epoch_3.h5\n",
    "...\n",
    "Decoded tokens: ['unfortunatly', 'ment', 'addressable', 'reley', 'dessicate']\n",
    "Target tokens:  ['unfortunatly', 'ment', 'addressable', 'relley', 'dessicate']\n",
    "-\n",
    "Saving full model to checkpoints/seq2seq_epoch_100.h5\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
